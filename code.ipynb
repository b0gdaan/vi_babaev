{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Q2LCbw3lQ4crDU3dE2z3okK3BAzQMquB","timestamp":1746269991478}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Блок 0: Установка зависимостей\n","# Устанавливаем все необходимые библиотеки\n","!pip install requests pandas pyspark pycryptodome\n","\n","# Блок 1: Подключение Google Диска и настройка окружения\n","# Подключаем Google Диск и создаем папку для хранения данных\n","from google.colab import drive\n","import os\n","\n","drive.mount('/content/drive')\n","output_dir = \"/content/drive/MyDrive/уник/cripto\"\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# Блок 2: Инициализация SQLite базы данных\n","# Создаем базу данных и таблицы для хранения криптовалют, дат и цен\n","import sqlite3\n","\n","conn = sqlite3.connect(os.path.join(output_dir, \"crypto.db\"))\n","cursor = conn.cursor()\n","\n","cursor.executescript('''\n","CREATE TABLE IF NOT EXISTS Cryptocurrency (\n","    crypto_id INTEGER PRIMARY KEY,\n","    name TEXT NOT NULL,\n","    ticker TEXT NOT NULL UNIQUE,\n","    description TEXT\n",");\n","CREATE TABLE IF NOT EXISTS Date (\n","    date_id INTEGER PRIMARY KEY,\n","    year INTEGER NOT NULL,\n","    month INTEGER NOT NULL,\n","    day INTEGER NOT NULL,\n","    UNIQUE(year, month, day)\n",");\n","CREATE TABLE IF NOT EXISTS PriceData (\n","    price_id INTEGER PRIMARY KEY,\n","    crypto_id INTEGER,\n","    date_id INTEGER,\n","    avg_price REAL,\n","    volume REAL,\n","    market_cap REAL,\n","    FOREIGN KEY (crypto_id) REFERENCES Cryptocurrency(crypto_id),\n","    FOREIGN KEY (date_id) REFERENCES Date(date_id)\n",");\n","''')\n","conn.commit()\n","\n","# Блок 3: Сбор данных с CryptoCompare API\n","# Запрашиваем исторические данные о ценах и объемах для BTC и ETH\n","import pandas as pd\n","import requests\n","from datetime import datetime, timedelta\n","import time\n","\n","def get_crypto_data(symbol, start_date, end_date, api_key=None):\n","    base_url = \"https://min-api.cryptocompare.com/data/v2/histoday\"\n","    days = (end_date - start_date).days\n","    params = {\n","        'fsym': symbol,\n","        'tsym': 'USD',\n","        'limit': min(days, 2000),\n","        'toTs': int(end_date.timestamp())\n","    }\n","    if api_key:\n","        params['api_key'] = api_key\n","\n","    response = requests.get(base_url, params=params)\n","    data = response.json()\n","    if data['Response'] != 'Success':\n","        raise ValueError(f\"Ошибка API: {data.get('Message', 'Неизвестная ошибка')}\")\n","\n","    df = pd.DataFrame(data['Data']['Data'])\n","    df['date'] = pd.to_datetime(df['time'], unit='s')\n","    df['avg_price'] = (df['high'] + df['low']) / 2\n","    df['volume'] = df['volumeto']\n","    df = df[['date', 'avg_price', 'volume']]\n","    return df[df['date'].notnull() & (df['avg_price'] > 0) & (df['date'] >= start_date)]\n","\n","coins = {'BTC': 'Bitcoin', 'ETH': 'Ethereum'}\n","start_date = datetime(2023, 1, 1)\n","end_date = datetime(2025, 1, 1)\n","all_data = []\n","\n","for symbol, name in coins.items():\n","    df = get_crypto_data(symbol, start_date, end_date)\n","    df['ticker'] = symbol\n","    all_data.append(df)\n","    time.sleep(1)\n","\n","full_df = pd.concat(all_data)\n","full_df['market_cap'] = 0.0  # CryptoCompare не предоставляет market_cap\n","full_df.to_csv(os.path.join(output_dir, \"crypto_raw_data.csv\"), index=False)\n","\n","# Блок 4: Агрегация данных с Apache Spark\n","# Выполняем месячную агрегацию цен и объемов\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import year, month, avg, col\n","\n","spark = SparkSession.builder.appName(\"CryptoAnalysis\").getOrCreate()\n","df_spark = spark.read.csv(os.path.join(output_dir, \"crypto_raw_data.csv\"), header=True, inferSchema=True)\n","\n","monthly_data = df_spark.groupBy(\n","    'ticker', year('date').alias('year'), month('date').alias('month')\n",").agg(\n","    avg('avg_price').alias('monthly_avg_price'),\n","    avg('volume').alias('monthly_avg_volume'),\n","    avg('market_cap').alias('monthly_avg_market_cap')\n",").filter(\n","    (col('year').between(2023, 2025)) & (col('month').between(1, 12))\n",").orderBy('ticker', 'year', 'month')\n","\n","monthly_data.write.csv(os.path.join(output_dir, \"crypto_monthly_data\"), mode=\"overwrite\")\n","\n","# Блок 5: Сохранение агрегированных данных в SQLite\n","# Записываем данные в таблицы Cryptocurrency, Date и PriceData\n","monthly_df = monthly_data.toPandas().dropna(subset=['year', 'month', 'ticker'])\n","monthly_df['year'] = monthly_df['year'].astype(int)\n","monthly_df['month'] = monthly_df['month'].astype(int)\n","\n","# Массовое добавление криптовалют\n","crypto_data = [(ticker, ticker) for ticker in monthly_df['ticker'].unique()]\n","cursor.executemany(\n","    \"INSERT OR IGNORE INTO Cryptocurrency (ticker, name) VALUES (?, ?)\",\n","    crypto_data\n",")\n","\n","# Массовое добавление дат\n","dates = monthly_df[['year', 'month']].drop_duplicates()\n","date_data = [(row['year'], row['month'], 1) for _, row in dates.iterrows()\n","             if 1 <= row['month'] <= 12 and 2023 <= row['year'] <= 2025]\n","cursor.executemany(\n","    \"INSERT OR IGNORE INTO Date (year, month, day) VALUES (?, ?, ?)\",\n","    date_data\n",")\n","conn.commit()\n","\n","# Получение словарей для crypto_id и date_id\n","cursor.execute(\"SELECT ticker, crypto_id FROM Cryptocurrency\")\n","crypto_ids = {row[0]: row[1] for row in cursor.fetchall()}\n","cursor.execute(\"SELECT year, month, date_id FROM Date\")\n","date_ids = {(row[0], row[1]): row[2] for row in cursor.fetchall()}\n","\n","# Массовое добавление цен\n","price_data = []\n","for _, row in monthly_df.iterrows():\n","    year, month = int(row['year']), int(row['month'])\n","    if (year, month) in date_ids and row['ticker'] in crypto_ids:\n","        price_data.append((\n","            crypto_ids[row['ticker']],\n","            date_ids[(year, month)],\n","            row['monthly_avg_price'],\n","            row['monthly_avg_volume'],\n","            row['monthly_avg_market_cap']\n","        ))\n","\n","cursor.executemany(\n","    \"INSERT INTO PriceData (crypto_id, date_id, avg_price, volume, market_cap) VALUES (?, ?, ?, ?, ?)\",\n","    price_data\n",")\n","conn.commit()\n","\n","# Блок 6: Шифрование файлов\n","# Шифруем CSV и базу данных с использованием AES\n","from Crypto.Cipher import AES\n","from Crypto.Random import get_random_bytes\n","\n","def encrypt_file(input_file, output_file, key):\n","    cipher = AES.new(key, AES.MODE_EAX)\n","    with open(input_file, 'rb') as f:\n","        data = f.read()\n","    ciphertext, tag = cipher.encrypt_and_digest(data)\n","    with open(output_file, 'wb') as f:\n","        [f.write(x) for x in (cipher.nonce, tag, ciphertext)]\n","\n","key = get_random_bytes(16)\n","with open(os.path.join(output_dir, \"key.txt\"), \"wb\") as f:\n","    f.write(key)\n","\n","encrypt_file(\n","    os.path.join(output_dir, \"crypto_raw_data.csv\"),\n","    os.path.join(output_dir, \"crypto_raw_data_encrypted.bin\"),\n","    key\n",")\n","encrypt_file(\n","    os.path.join(output_dir, \"crypto.db\"),\n","    os.path.join(output_dir, \"crypto_encrypted.db\"),\n","    key\n",")\n","\n","# Блок 7: Вывод результатов\n","# Выполняем SQL-запрос для отображения агрегированных цен\n","cursor.execute('''\n","SELECT c.ticker, d.year, d.month, p.avg_price\n","FROM PriceData p\n","JOIN Cryptocurrency c ON p.crypto_id = c.crypto_id\n","JOIN Date d ON p.date_id = d.date_id\n","ORDER BY c.ticker, d.year, d.month\n","''')\n","results = cursor.fetchall()\n","\n","for row in results:\n","    print(f\"{row[0]} {row[1]}-{row[2]}: ${row[3]:.2f}\")\n","\n","conn.close()\n","\n","# Блок 8: Очистка\n","# Останавливаем Spark и синхронизируем Google Диск\n","spark.stop()\n","drive.flush_and_unmount()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VQORRBY3uVjK","executionInfo":{"status":"ok","timestamp":1746438210948,"user_tz":-120,"elapsed":74024,"user":{"displayName":"Богдан Бабаев","userId":"14239075785960973133"}},"outputId":"0daac98d-3ebc-4993-b066-f7420aac3ab9"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n","Collecting pycryptodome\n","  Downloading pycryptodome-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n","Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Downloading pycryptodome-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pycryptodome\n","Successfully installed pycryptodome-3.22.0\n","Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Подключение к Google Диску и базе данных\n","from google.colab import drive\n","import sqlite3\n","import os\n","import pandas as pd\n","import glob\n","\n","# Монтирование Google Диска\n","drive.mount('/content/drive', force_remount=True)\n","\n","# Путь к базе данных и создание папки\n","output_dir = \"/content/drive/MyDrive/уник/cripto\"\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# Проверка существования базы данных\n","db_path = os.path.join(output_dir, \"crypto.db\")\n","if not os.path.exists(db_path):\n","    print(f\"Ошибка: Файл базы данных {db_path} не существует.\")\n","    print(\"Выполните основной скрипт (crypto_data_processing.py) заново.\")\n","else:\n","    print(f\"База данных найдена: {db_path}\")\n","\n","# Диагностика файлов, созданных основным скриптом\n","print(\"\\n=== Диагностика файлов ===\")\n","print(\"Файлы в директории:\", os.listdir(output_dir))\n","\n","# Проверка crypto_raw_data.csv\n","raw_data_path = os.path.join(output_dir, \"crypto_raw_data.csv\")\n","if os.path.exists(raw_data_path):\n","    raw_df = pd.read_csv(raw_data_path)\n","    print(\"\\nСодержимое crypto_raw_data.csv:\")\n","    print(f\"Количество записей: {len(raw_df)}\")\n","    print(f\"Диапазон дат: {raw_df['date'].min()} - {raw_df['date'].max()}\")\n","    print(f\"Тикеры: {raw_df['ticker'].unique()}\")\n","else:\n","    print(f\"Файл {raw_data_path} не найден.\")\n","\n","# Проверка crypto_monthly_data\n","monthly_data_path = os.path.join(output_dir, \"crypto_monthly_data\")\n","if os.path.exists(monthly_data_path):\n","    monthly_files = glob.glob(os.path.join(monthly_data_path, \"part-*.csv\"))\n","    if monthly_files:\n","        monthly_df = pd.concat([pd.read_csv(f) for f in monthly_files])\n","        print(\"\\nСодержимое crypto_monthly_data:\")\n","        print(f\"Количество записей: {len(monthly_df)}\")\n","    else:\n","        print(\"Нет CSV-файлов в crypto_monthly_data.\")\n","else:\n","    print(f\"Папка {monthly_data_path} не найдена.\")\n","\n","# Подключение к базе данных\n","try:\n","    conn = sqlite3.connect(db_path)\n","    cursor = conn.cursor()\n","\n","    # Диагностика: Проверка структуры и содержимого базы\n","    print(\"\\n=== Диагностика базы данных ===\")\n","\n","    # Список таблиц\n","    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n","    tables = cursor.fetchall()\n","    print(\"Таблицы в базе:\", tables)\n","\n","    # Количество записей в каждой таблице\n","    if tables:\n","        cursor.execute(\"SELECT count(*) FROM Cryptocurrency\")\n","        crypto_count = cursor.fetchone()[0]\n","        print(\"Количество записей в Cryptocurrency:\", crypto_count)\n","\n","        cursor.execute(\"SELECT count(*) FROM Date\")\n","        date_count = cursor.fetchone()[0]\n","        print(\"Количество записей в Date:\", date_count)\n","\n","        cursor.execute(\"SELECT count(*) FROM PriceData\")\n","        price_count = cursor.fetchone()[0]\n","        print(\"Количество записей в PriceData:\", price_count)\n","\n","        # Пример содержимого PriceData (первые 5 записей)\n","        if price_count > 0:\n","            cursor.execute(\"\"\"\n","            SELECT c.ticker, d.year, d.month, p.avg_price, p.volume\n","            FROM PriceData p\n","            JOIN Cryptocurrency c ON p.crypto_id = c.crypto_id\n","            JOIN Date d ON p.date_id = d.date_id\n","            LIMIT 5\n","            \"\"\")\n","            sample_data = cursor.fetchall()\n","            print(\"\\nПример данных из PriceData (первые 5 записей):\")\n","            for row in sample_data:\n","                print(f\"Ticker: {row[0]}, {row[1]}-{row[2]}: Price=${row[3]:.2f}, Volume=${row[4]:.2f}\")\n","        else:\n","            print(\"\\nТаблица PriceData пуста.\")\n","    else:\n","        print(\"В базе данных нет таблиц.\")\n","\n","    # SQL-запрос 1: Получение списка всех криптовалют\n","    print(\"\\n=== SQL-запрос 1: Список всех криптовалют ===\")\n","    try:\n","        cursor.execute(\"SELECT ticker, name, description FROM Cryptocurrency\")\n","        results = cursor.fetchall()\n","        if results:\n","            for row in results:\n","                print(f\"Ticker: {row[0]}, Name: {row[1]}, Description: {row[2]}\")\n","        else:\n","            print(\"Нет данных в таблице Cryptocurrency\")\n","    except sqlite3.OperationalError as e:\n","        print(f\"Ошибка при выполнении запроса: {e}\")\n","\n","    # SQL-запрос 2: Средние месячные цены для Bitcoin в 2023 году\n","    print(\"\\n=== SQL-запрос 2: Средние цены Bitcoin за 2023 год ===\")\n","    try:\n","        cursor.execute(\"\"\"\n","        SELECT d.year, d.month, p.avg_price\n","        FROM PriceData p\n","        JOIN Cryptocurrency c ON p.crypto_id = c.crypto_id\n","        JOIN Date d ON p.date_id = d.date_id\n","        WHERE c.ticker = 'BTC' AND d.year = 2023\n","        ORDER BY d.year, d.month\n","        \"\"\")\n","        results = cursor.fetchall()\n","        if results:\n","            for row in results:\n","                print(f\"{row[0]}-{row[1]:02d}: ${row[2]:.2f}\")\n","        else:\n","            print(\"Нет данных для Bitcoin за 2023 год\")\n","    except sqlite3.OperationalError as e:\n","        print(f\"Ошибка при выполнении запроса: {e}\")\n","\n","    # SQL-запрос 3: Максимальная и минимальная цена для каждой криптовалюты\n","    print(\"\\n=== SQL-запрос 3: Максимальная и минимальная цена для каждой криптовалюты ===\")\n","    try:\n","        cursor.execute(\"\"\"\n","        SELECT c.ticker, MAX(p.avg_price) as max_price, MIN(p.avg_price) as min_price\n","        FROM PriceData p\n","        JOIN Cryptocurrency c ON p.crypto_id = c.crypto_id\n","        GROUP BY c.ticker\n","        \"\"\")\n","        results = cursor.fetchall()\n","        if results:\n","            for row in results:\n","                print(f\"{row[0]}: Max = ${row[1]:.2f}, Min = ${row[2]:.2f}\")\n","        else:\n","            print(\"Нет данных о ценах в таблице PriceData\")\n","    except sqlite3.OperationalError as e:\n","        print(f\"Ошибка при выполнении запроса: {e}\")\n","\n","    # SQL-запрос 4: Общий объем торгов Ethereum за весь период\n","    print(\"\\n=== SQL-запрос 4: Общий объем торгов Ethereum за весь период ===\")\n","    try:\n","        cursor.execute(\"\"\"\n","        SELECT SUM(p.volume) as total_volume\n","        FROM PriceData p\n","        JOIN Cryptocurrency c ON p.crypto_id = c.crypto_id\n","        JOIN Date d ON p.date_id = d.date_id\n","        WHERE c.ticker = 'ETH'\n","        \"\"\")\n","        result = cursor.fetchone()\n","        if result and result[0] is not None:\n","            print(f\"Total Volume: ${result[0]:.2f}\")\n","        else:\n","            print(\"Нет данных об объеме торгов Ethereum\")\n","    except sqlite3.OperationalError as e:\n","        print(f\"Ошибка при выполнении запроса: {e}\")\n","\n","    # SQL-запрос 5: Месяцы с наибольшим объемом торгов для BTC\n","    print(\"\\n=== SQL-запрос 5: Месяцы с наибольшим объемом торгов для BTC ===\")\n","    try:\n","        cursor.execute(\"\"\"\n","        SELECT c.ticker, d.year, d.month, p.volume\n","        FROM PriceData p\n","        JOIN Cryptocurrency c ON p.crypto_id = c.crypto_id\n","        JOIN Date d ON p.date_id = d.date_id\n","        WHERE c.ticker = 'BTC'\n","        ORDER BY p.volume DESC\n","        LIMIT 5\n","        \"\"\")\n","        results = cursor.fetchall()\n","        if results:\n","            for row in results:\n","                print(f\"{row[0]} {row[1]}-{row[2]:02d}: Volume=${row[3]:.2f}\")\n","        else:\n","            print(\"Нет данных об объеме торгов Bitcoin\")\n","    except sqlite3.OperationalError as e:\n","        print(f\"Ошибка при выполнении запроса: {e}\")\n","\n","    # Закрытие соединения\n","    conn.close()\n","\n","except sqlite3.OperationalError as e:\n","    print(f\"Ошибка подключения к базе данных: {e}\")\n","    print(\"Убедитесь, что crypto.db существует и доступен.\")\n","\n","# Синхронизация Google Диска\n","drive.flush_and_unmount()"],"metadata":{"id":"COQzJIP6ykS9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746438239981,"user_tz":-120,"elapsed":5239,"user":{"displayName":"Богдан Бабаев","userId":"14239075785960973133"}},"outputId":"6fc2a656-82ba-40f1-e154-5e9e3de6ba78"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","База данных найдена: /content/drive/MyDrive/уник/cripto/crypto.db\n","\n","=== Диагностика файлов ===\n","Файлы в директории: ['crypto.db', 'crypto_raw_data.csv', 'crypto_monthly_data', 'key.txt', 'crypto_raw_data_encrypted.bin', 'crypto_encrypted.db']\n","\n","Содержимое crypto_raw_data.csv:\n","Количество записей: 1464\n","Диапазон дат: 2023-01-01 - 2025-01-01\n","Тикеры: ['BTC' 'ETH']\n","\n","Содержимое crypto_monthly_data:\n","Количество записей: 49\n","\n","=== Диагностика базы данных ===\n","Таблицы в базе: [('Cryptocurrency',), ('Date',), ('PriceData',)]\n","Количество записей в Cryptocurrency: 2\n","Количество записей в Date: 25\n","Количество записей в PriceData: 0\n","\n","Таблица PriceData пуста.\n","\n","=== SQL-запрос 1: Список всех криптовалют ===\n","Ticker: BTC, Name: BTC, Description: None\n","Ticker: ETH, Name: ETH, Description: None\n","\n","=== SQL-запрос 2: Средние цены Bitcoin за 2023 год ===\n","Нет данных для Bitcoin за 2023 год\n","\n","=== SQL-запрос 3: Максимальная и минимальная цена для каждой криптовалюты ===\n","Нет данных о ценах в таблице PriceData\n","\n","=== SQL-запрос 4: Общий объем торгов Ethereum за весь период ===\n","Нет данных об объеме торгов Ethereum\n","\n","=== SQL-запрос 5: Месяцы с наибольшим объемом торгов для BTC ===\n","Нет данных об объеме торгов Bitcoin\n"]}]},{"cell_type":"code","source":["import pyspark.sql.functions as F\n","from pyspark.sql import SparkSession\n","from pyspark.sql.window import Window\n","from pyspark.ml.feature import VectorAssembler, StandardScaler\n","from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n","from pyspark.ml import Pipeline\n","from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import os\n","from google.colab import drive\n","\n","# Монтирование Google Drive (для Google Colab)\n","drive.mount('/content/drive')\n","\n","# Инициализация Spark-сессии\n","spark = SparkSession.builder.appName(\"CryptoPricePrediction\").getOrCreate()\n","\n","# Указание пути к файлу\n","file_path = \"/content/drive/MyDrive/уник/cripto/crypto_raw_data.csv\"\n","\n","# Проверка существования файла\n","if not os.path.exists(file_path):\n","    raise FileNotFoundError(f\"Файл не найден по пути: {file_path}. Пожалуйста, проверьте путь и убедитесь, что файл существует.\")\n","\n","# Загрузка данных\n","df = spark.read.csv(file_path, header=True, inferSchema=True)\n","\n","# Очистка данных\n","# Удаляем строки с пропусками и колонку market_cap\n","df = df.dropna().drop(\"market_cap\")\n","\n","# Фильтрация данных для BTC (можно повторить для ETH)\n","df_btc = df.filter(df.ticker == \"BTC\")\n","\n","# Инженерия признаков\n","# Сортировка по дате\n","df_btc = df_btc.orderBy(\"date\")\n","\n","# Создание признаков: процентное изменение цены, скользящее среднее, волатильность\n","df_btc = df_btc.withColumn(\"price_change\",\n","    (F.col(\"avg_price\") - F.lag(\"avg_price\").over(Window.partitionBy(\"ticker\").orderBy(\"date\"))) /\n","    F.lag(\"avg_price\").over(Window.partitionBy(\"ticker\").orderBy(\"date\")))\n","\n","# Скользящее среднее за 7 дней\n","df_btc = df_btc.withColumn(\"ma7\",\n","    F.avg(\"avg_price\").over(Window.partitionBy(\"ticker\").orderBy(\"date\").rowsBetween(-6, 0)))\n","\n","# Волатильность (стандартное отклонение за 7 дней)\n","df_btc = df_btc.withColumn(\"volatility\",\n","    F.stddev(\"avg_price\").over(Window.partitionBy(\"ticker\").orderBy(\"date\").rowsBetween(-6, 0)))\n","\n","# Логарифм объема торгов\n","df_btc = df_btc.withColumn(\"log_volume\", F.log1p(F.col(\"volume\")))\n","\n","# Целевая переменная: 1 если цена вырастет на следующий день, 0 иначе\n","df_btc = df_btc.withColumn(\"label\",\n","    F.when(F.lead(\"avg_price\").over(Window.partitionBy(\"ticker\").orderBy(\"date\")) > F.col(\"avg_price\"), 1).otherwise(0))\n","\n","# Удаление строк с пропусками после создания признаков\n","df_btc = df_btc.dropna()\n","\n","# Разделение на тренировочную и тестовую выборки\n","train_df, test_df = df_btc.randomSplit([0.8, 0.2], seed=42)\n","\n","# Подготовка данных для модели\n","feature_columns = [\"price_change\", \"ma7\", \"volatility\", \"log_volume\"]\n","assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features_raw\")\n","scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\", withStd=True, withMean=True)\n","\n","# Определение моделей\n","lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\")\n","rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=100)\n","gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=50)\n","\n","# Создание пайплайнов\n","pipeline_lr = Pipeline(stages=[assembler, scaler, lr])\n","pipeline_rf = Pipeline(stages=[assembler, scaler, rf])\n","pipeline_gbt = Pipeline(stages=[assembler, scaler, gbt])\n","\n","# Обучение моделей\n","model_lr = pipeline_lr.fit(train_df)\n","model_rf = pipeline_rf.fit(train_df)\n","model_gbt = pipeline_gbt.fit(train_df)\n","\n","# Оценка моделей\n","evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n","\n","# Предсказания и оценка на тестовой выборке\n","pred_lr = model_lr.transform(test_df)\n","pred_rf = model_rf.transform(test_df)\n","pred_gbt = model_gbt.transform(test_df)\n","\n","roc_lr = evaluator.evaluate(pred_lr)\n","roc_rf = evaluator.evaluate(pred_rf)\n","roc_gbt = evaluator.evaluate(pred_gbt)\n","\n","print(f\"ROC-AUC Logistic Regression: {roc_lr}\")\n","print(f\"ROC-AUC Random Forest: {roc_rf}\")\n","print(f\"ROC-AUC Gradient Boosting: {roc_gbt}\")\n","\n","# Визуализация цен и предсказаний\n","# Конвертация в Pandas для визуализации\n","pdf = df_btc.select(\"date\", \"avg_price\", \"label\").toPandas()\n","pred_pdf = pred_rf.select(\"date\", \"prediction\").toPandas()\n","\n","# Объединение данных\n","pdf = pdf.merge(pred_pdf, on=\"date\", how=\"inner\")\n","\n","# Построение графика\n","plt.figure(figsize=(12, 6))\n","plt.plot(pdf[\"date\"], pdf[\"avg_price\"], label=\"BTC Price\", color=\"blue\")\n","plt.scatter(pdf[pdf[\"prediction\"] == 1][\"date\"], pdf[pdf[\"prediction\"] == 1][\"avg_price\"],\n","            color=\"green\", label=\"Predicted Up\", marker=\"^\")\n","plt.scatter(pdf[pdf[\"prediction\"] == 0][\"date\"], pdf[pdf[\"prediction\"] == 0][\"avg_price\"],\n","            color=\"red\", label=\"Predicted Down\", marker=\"v\")\n","plt.xlabel(\"Date\")\n","plt.ylabel(\"Average Price (USD)\")\n","plt.title(\"BTC Price with Predicted Up/Down Movements\")\n","plt.legend()\n","plt.grid(True)\n","plt.savefig(\"btc_price_prediction.png\")\n","plt.close()\n","\n","# Остановка Spark-сессии\n","spark.stop()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KT7nJPU6G0i2","executionInfo":{"status":"ok","timestamp":1746438324238,"user_tz":-120,"elapsed":76870,"user":{"displayName":"Богдан Бабаев","userId":"14239075785960973133"}},"outputId":"0129a8ab-800a-4f2e-d7bb-a4e5d68c77fe"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","ROC-AUC Logistic Regression: 0.6381045115222331\n","ROC-AUC Random Forest: 0.5689711132749109\n","ROC-AUC Gradient Boosting: 0.4589419019798766\n"]}]},{"cell_type":"markdown","source":["\n","ROC-AUC Logistic Regression: 0.6381045115222331\n","ROC-AUC Random Forest: 0.5657254138266797\n","ROC-AUC Gradient Boosting: 0.4589419019798766"],"metadata":{"id":"GQAq8gTHH3IE"}},{"cell_type":"code","source":["df_btc.groupBy(\"label\").count().show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":287},"id":"q_0Uar-cIoXl","executionInfo":{"status":"error","timestamp":1746275717704,"user_tz":-120,"elapsed":30,"user":{"displayName":"Bo B","userId":"06853673128116613236"}},"outputId":"c4a04174-258f-4923-afee-e703c4802f35"},"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"SparkContext or SparkSession should be created first.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-d0ab0543bfb3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_btc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mgroupBy\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3414\u001b[0m         \u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3415\u001b[0m         \"\"\"\n\u001b[0;32m-> 3416\u001b[0;31m         \u001b[0mjgd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3417\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGroupedData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_jcols\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   2764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2765\u001b[0m             \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2766\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_to_java_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2768\u001b[0m     def _sort_cols(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_jseq\u001b[0;34m(self, cols, converter)\u001b[0m\n\u001b[1;32m   2751\u001b[0m     ) -> JavaObject:\n\u001b[1;32m   2752\u001b[0m         \u001b[0;34m\"\"\"Return a JVM Seq of Columns from a list of Column or names\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2753\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_jmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJavaObject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_seq\u001b[0;34m(sc, cols, converter)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \"\"\"\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \"\"\"\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_java_column\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mjcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mjcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_column_from_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         raise PySparkTypeError(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_create_column_from_name\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_create_column_from_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Column\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_active_spark_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJVMView\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mget_active_spark_context\u001b[0;34m()\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SparkContext or SparkSession should be created first.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: SparkContext or SparkSession should be created first."]}]},{"cell_type":"code","source":["df_btc.select(\"price_change\", \"ma7\", \"volatility\", \"log_volume\").summary().show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":287},"id":"_AzRJOQ_JNFV","executionInfo":{"status":"error","timestamp":1746275739724,"user_tz":-120,"elapsed":68,"user":{"displayName":"Bo B","userId":"06853673128116613236"}},"outputId":"7e2ba4a9-5079-4454-8f42-16f64a9569ed"},"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"SparkContext or SparkSession should be created first.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-1e0e02c9b396>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_btc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"price_change\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ma7\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"volatility\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"log_volume\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3227\u001b[0m         \u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3228\u001b[0m         \"\"\"\n\u001b[0;32m-> 3229\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_jcols\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   2764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2765\u001b[0m             \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2766\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_to_java_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2768\u001b[0m     def _sort_cols(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_jseq\u001b[0;34m(self, cols, converter)\u001b[0m\n\u001b[1;32m   2751\u001b[0m     ) -> JavaObject:\n\u001b[1;32m   2752\u001b[0m         \u001b[0;34m\"\"\"Return a JVM Seq of Columns from a list of Column or names\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2753\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_jmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJavaObject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_seq\u001b[0;34m(sc, cols, converter)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \"\"\"\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \"\"\"\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_java_column\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mjcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mjcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_column_from_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         raise PySparkTypeError(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_create_column_from_name\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_create_column_from_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Column\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_active_spark_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJVMView\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mget_active_spark_context\u001b[0;34m()\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SparkContext or SparkSession should be created first.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: SparkContext or SparkSession should be created first."]}]},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","# Создаем SparkSession\n","spark = SparkSession.builder \\\n","    .appName(\"CryptoAnalysis\") \\\n","    .getOrCreate()\n","\n","# Теперь можно читать данные и работать с DataFrame\n","df_btc = spark.read.csv(\"/content/drive/MyDrive/уник/cripto/crypto_raw_data.csv\", header=True, inferSchema=True)\n","\n","# Пример: показать статистику\n","df_btc.select(\"price_change\", \"ma7\", \"volatility\", \"log_volume\").summary().show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"bA4gkTMBJSc8","executionInfo":{"status":"error","timestamp":1746275813054,"user_tz":-120,"elapsed":374,"user":{"displayName":"Bo B","userId":"06853673128116613236"}},"outputId":"eac26462-b5d2-4d90-ad42-7460dc1614d7"},"execution_count":null,"outputs":[{"output_type":"error","ename":"AnalysisException","evalue":"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `price_change` cannot be resolved. Did you mean one of the following? [`market_cap`, `ticker`, `avg_price`, `date`, `volume`].;\n'Project ['price_change, 'ma7, 'volatility, 'log_volume]\n+- Relation [date#1366,avg_price#1367,volume#1368,ticker#1369,market_cap#1370] csv\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-cfb24ae182ce>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Пример: показать статистику\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdf_btc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"price_change\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ma7\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"volatility\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"log_volume\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3227\u001b[0m         \u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3228\u001b[0m         \"\"\"\n\u001b[0;32m-> 3229\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `price_change` cannot be resolved. Did you mean one of the following? [`market_cap`, `ticker`, `avg_price`, `date`, `volume`].;\n'Project ['price_change, 'ma7, 'volatility, 'log_volume]\n+- Relation [date#1366,avg_price#1367,volume#1368,ticker#1369,market_cap#1370] csv\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"_m3PqOY6Jdv6"},"execution_count":null,"outputs":[]}]}